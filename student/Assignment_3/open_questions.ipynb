{
  "cells": [
    {
      "cell_type": "raw",
      "id": "e6959b8c",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Assignment 3: Sentiment Classification Reflection\"\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    toc-title: Contents\n",
        "    toc-depth: 4\n",
        "    self-contained: true\n",
        "    number-sections: false\n",
        "jupyter: python3\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ac42e4b",
      "metadata": {},
      "source": [
        "## Results Summary\n",
        "\n",
        "The table below summarizes test-set performance for all six models on the same stratified split. I have also put some key plots in the end of notebook.\n",
        "\n",
        "| Model | Test Accuracy | Test Macro F1 |\n",
        "|---|---:|---:|\n",
        "| MLP | 0.7400 | 0.6812 |\n",
        "| RNN | 0.6988 | 0.6812 |\n",
        "| LSTM | 0.7689 | 0.7458 |\n",
        "| GRU | 0.7634 | 0.7330 |\n",
        "| BERT | 0.8171 | 0.8033 |\n",
        "| GPT | 0.8033 | 0.7815 |\n",
        "\n",
        "Ranking by macro F1 (best to worst): **BERT > GPT > LSTM > GRU > MLP = RNN**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50ddc09b",
      "metadata": {},
      "source": [
        "## 1. Training Dynamics\n",
        "\n",
        "### Did your models show signs of overfitting or underfitting? What changes could address this?\n",
        "\n",
        "Both MLP and LSTM showed **overfitting**, but the LSTM showed it more strongly.\n",
        "\n",
        "- **MLP:** Training metrics improved steadily, while validation F1 plateaued around 0.66-0.69 after early epochs. This is moderate overfitting.\n",
        "- **LSTM:** Training macro F1 became very high (close to perfect) while validation F1 peaked around 0.76 and then flattened or fluctuated. This is stronger overfitting.\n",
        "\n",
        "Changes that could help:\n",
        "- Increase regularization.\n",
        "- Use earlier stopping.\n",
        "- Reduce model capacity.\n",
        "- Try data augmentation or noise injection.\n",
        "\n",
        "### How did class weights affect training stability and final performance?\n",
        "\n",
        "Using `CrossEntropyLoss` helped the models pay more attention to minority classes (especially negative and positive), instead of being dominated by the neutral class. This generally improved macro F1 and class balance, but also made optimization noisier early in training, which is expected when minority-class errors receive higher gradient weight."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcfb6ba1",
      "metadata": {},
      "source": [
        "## 2. Model Performance and Error Analysis\n",
        "\n",
        "### Which of your two models generalized better to the test set?\n",
        "\n",
        "**LSTM generalized better than MLP.**\n",
        "\n",
        "- MLP test macro F1: **0.6812**\n",
        "- LSTM test macro F1: **0.7458**\n",
        "\n",
        "The LSTM also had higher test accuracy (0.7689 vs. 0.7400).\n",
        "\n",
        "### Which sentiment class was most frequently misclassified, and why?\n",
        "\n",
        "For both MLP and LSTM, the **positive class** had the lowest per-class F1:\n",
        "\n",
        "- MLP: Positive F1 = **0.5969**\n",
        "- LSTM: Positive F1 = **0.6425**\n",
        "\n",
        "Likely reasons:\n",
        "- Positive financial statements are often subtle and can be linguistically close to neutral statements.\n",
        "- The neutral class is the largest class, so borderline examples are often pulled toward neutral predictions.\n",
        "- Mean pooling (MLP) especially loses local contextual cues and sentiment shifters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b989de",
      "metadata": {},
      "source": [
        "## 3. Cross-Model Comparison\n",
        "\n",
        "### How did mean-pooled FastText embeddings limit the MLP vs. sequence models?\n",
        "\n",
        "Mean pooling removes token order and long-distance dependencies. The MLP receives one fixed sentence vector and cannot model sequence structure, negation scope, or phrase-level interactions as effectively as sequence models.\n",
        "\n",
        "### What advantage did LSTM sequential processing provide over MLP?\n",
        "\n",
        "The LSTM processed tokens in order and used hidden states to retain contextual information across the sentence. This improved test macro F1 from **0.6812 (MLP)** to **0.7458 (LSTM)**.\n",
        "\n",
        "### Did fine-tuned LLMs (BERT/GPT) outperform classical baselines? Why?\n",
        "\n",
        "Yes. Both BERT and GPT outperformed all classical baselines:\n",
        "\n",
        "- BERT macro F1 = **0.8033**\n",
        "- GPT macro F1 = **0.7815**\n",
        "\n",
        "Pretraining gives these models rich contextual representations and language knowledge that static embedding pipelines do not capture. Fine-tuning then adapts those representations to the financial sentiment task efficiently.\n",
        "\n",
        "### Rank all six models by test performance and explain the ranking\n",
        "\n",
        "By test macro F1:\n",
        "1. **BERT (0.8033)**\n",
        "2. **GPT (0.7815)**\n",
        "3. **LSTM (0.7458)**\n",
        "4. **GRU (0.7330)**\n",
        "5. **MLP (0.6812)**\n",
        "6. **RNN (0.6812)**\n",
        "\n",
        "Why:\n",
        "- Transformer LLMs are strongest due to contextual pretraining and expressive architecture.\n",
        "- LSTM/GRU outperform MLP/RNN because gated recurrent units handle sequence dependencies better and mitigate vanishing-gradient issues better than vanilla RNNs.\n",
        "- MLP and vanilla RNN are weakest due to representational limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f752c2b6",
      "metadata": {},
      "source": "## Key Plots\n\n### MLP\n![](outputs/mlp_f1_learning_curves.png)\n### LSTM\n![](outputs/lstm_f1_learning_curves.png)\n### RNN / GRU / BERT / GPT\n![](outputs/rnn_f1_learning_curves.png)\n![](outputs/gru_f1_learning_curves.png)\n![](outputs/bert_f1_learning_curves.png)\n![](outputs/gpt_f1_learning_curves.png)"
    },
    {
      "cell_type": "markdown",
      "id": "6623739f",
      "metadata": {},
      "source": [
        "## AI Use Disclosure (Required)\n",
        "\n",
        "## No\n",
        "\n",
        "- **Tool(s) used:** OpenAI Codex (GPT-5 based coding assistant in this environment).\n",
        "- **How it was used:** Implemented `train_sentiment_mlp_classifier.py` and `train_sentiment_lstm_classifier.py`, helped execute all required training scripts, and drafted this reflection text using observed results.\n",
        "- **What I verified myself:** I ran all six training scripts, confirmed output figures/model checkpoints were generated, and checked final test metrics before writing the analysis and ranking."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "stat359-su25-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}