{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3f9655a7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Assignment 3: Neural Models for Sentiment Classification\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    self-contained: true\n",
    "    number-sections: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43e1df",
   "metadata": {},
   "source": [
    "### <font color='blue'> Due 11:59pm, Monday Feb 12th 2026</font>\n",
    "\n",
    "**Purpose / learning goals:**\n",
    "- Practice training neural models in PyTorch with emphasis on optimizers, regularization, and learning-rate scheduling to meet a performance threshold.\n",
    "- Use sentiment classification as a downstream task to compare classical neural baselines with fine-tuned pretrained LLMs (BERT/GPT).\n",
    "\n",
    "**Runtime / setup notes:**\n",
    "- This assignment does not require a GPU to train the models. Using a GPU (or Apple MPS) will usually speed up training for the transformer models.\n",
    "\n",
    "In this assignment, you will:\n",
    "- Implement MLP and LSTM classifiers (your code)\n",
    "- Run provided scripts for RNN, GRU, BERT, and GPT (for comparison)\n",
    "\n",
    "**Implementation format:** Task 1 and Task 2 must be implemented as Python scripts (not notebooks). The open-ended questions are answered in a notebook.\n",
    "\n",
    "To motivate the transformer architecture, scripts are provided for pretrained state-of-the-art models such as **GPT** (decoder-only) and **BERT** (encoder-only). You should run these scripts yourself to obtain results for comparison and reflection.\n",
    "\n",
    "*Please read the `README.md` file before proceeding.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e243326",
   "metadata": {},
   "source": [
    "##  Sentiment Classification: Classical Nets vs. LLMs\n",
    "\n",
    "Sentiment classification is a common **downstream task** for evaluating how well pretrained LLMs adapt to a domain via fine-tuning, compared against classical neural baselines.\n",
    "\n",
    "In this assignment, you'll explore how different neural architectures perform on sentiment classification:\n",
    "\n",
    "- **Classical approaches:** MLP, RNN, LSTM, GRU (using static FastText embeddings)\n",
    "- **Pretrained LLMs:** BERT and GPT (fine-tuned using Hugging Face Transformers)\n",
    "\n",
    "You will implement MLP and LSTM yourself; scripts are provided for the remaining models.\n",
    "\n",
    "Detailed requirements for your implementations are listed in **Your Tasks** below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d347c96",
   "metadata": {},
   "source": [
    "##  Dataset: Financial PhraseBank\n",
    "\n",
    "This assignment uses the **Financial PhraseBank** dataset, developed by  \n",
    "Mika V. MÃ¤ntylÃ¤, Graziella Linders, Tanja Suominen, and Miikka Kuutila.\n",
    "\n",
    "- ðŸ“‚ Dataset homepage: [Hugging Face â€“ Financial_PhraseBank](https://huggingface.co/datasets/takala/financial_phrasebank)  \n",
    "- ðŸ“„ Original paper:  \n",
    "  P. Malo, A. Sinha, et al. (2014). [*â€œGood Debt or Bad Debt: Detecting Semantic Orientations in Economic Textsâ€*](https://arxiv.org/pdf/1307.5336)\n",
    "\n",
    "You can load and preview the dataset using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea9197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n========== Loading Dataset ==========\")\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('financial_phrasebank', 'sentences_50agree', trust_remote_code=True)\n",
    "print(\"Dataset loaded. Example:\", dataset['train'][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51c574",
   "metadata": {},
   "source": [
    "###  Dataset Description\n",
    "\n",
    "The dataset consists of **4,840 English sentences** extracted from financial news articles.  \n",
    "Each sentence is labeled as **positive**, **neutral**, or **negative**, with annotations provided by 5 to 8 human annotators to ensure labeling consistency.  \n",
    "\n",
    "This assignment uses the `'sentences_50agree'` subset, where at least 50% of annotators agreed on the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05e0ad",
   "metadata": {},
   "source": [
    "###  Class Imbalance\n",
    "\n",
    "The dataset has an **imbalanced class distribution**:\n",
    "\n",
    "| Sentiment | Count |\n",
    "|-----------|-------|\n",
    "| Negative  | 604   |\n",
    "| Neutral   | 2879  |\n",
    "| Positive  | 1363  |\n",
    "\n",
    "For dealing with imbalanced dataset:\n",
    "\n",
    "- **Accuracy** can be misleading in this setting.\n",
    "- You must use `class_weight` in your loss function (e.g., `nn.CrossEntropyLoss(weight=...)`) to mitigate the imbalance.\n",
    "- The primary evaluation metric will be the **macro-averaged F1 score**, which treats all classes equally regardless of frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf5ce4c",
   "metadata": {},
   "source": [
    "### Train/Validation/Test Splits\n",
    "\n",
    "The dataset does **not** come with predefined splits.\n",
    "\n",
    "You must split it yourself using **stratified sampling** to preserve class proportions in each subset.\n",
    "\n",
    "For a fair comparison and to stay consistent with the other model scripts, use the following split procedure:\n",
    "\n",
    "- First, create a **test set (15%)** and a **train+validation set (85%)** using stratified sampling on the original labels.\n",
    "- Then, split the **train+validation set** into **training (85%)** and **validation (15%)** using stratified sampling on the train+validation labels.\n",
    "- Use a fixed random seed (e.g., 42) so results are reproducible.\n",
    "\n",
    "This ensures consistent and representative evaluation, especially in the presence of class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b696bf",
   "metadata": {},
   "source": [
    "## Your Tasks\n",
    "\n",
    "Before you begin, please follow these best practices in your implementation:\n",
    "\n",
    "- Set **random seeds** to ensure reproducibility  \n",
    "- Use `torch.save()` to save your **best-performing model**  \n",
    "- Modularize your code into **reusable functions or classes**\n",
    "\n",
    "You are encouraged to experiment with different **neural network architectures**, **hyperparameters**, **optimizers**, **regularization** (e.g., dropout, weight decay), and **learning-rate scheduling**, as long as your final model meets the required **macro F1 score threshold** for each task.\n",
    "\n",
    "**Implementation format:** Task 1 and Task 2 must be implemented as Python scripts (not notebooks). Name them as specified below.\n",
    "\n",
    "### Task 1: MLP with Mean-Pooled FastText Sentence Embedding **(25 points)**\n",
    "\n",
    "Create a script named `train_sentiment_mlp_classifier.py` and complete the following:\n",
    "\n",
    "- Load **pretrained FastText embeddings** using Gensim.\n",
    "- Tokenize each sentence and compute the **mean of its word vectors** to obtain a fixed-size (300-dimensional) sentence embedding.\n",
    "- Use a **Multi-Layer Perceptron (MLP)** to classify the sentence embedding.\n",
    "- Handle **class imbalance** using `nn.CrossEntropyLoss(weight=...)`.\n",
    "- Track and report the following metrics:\n",
    "  - **Loss**\n",
    "  - **Accuracy**\n",
    "  - **Macro F1 Score**\n",
    "\n",
    "#### Performance Requirement:\n",
    "Your model must achieve a **Test Macro F1 Score >= 0.65**\n",
    "\n",
    "### Task 2: LSTM with Padded FastText Word Vectors **(25 points)**\n",
    "\n",
    "Create a script named `train_sentiment_lstm_classifier.py` and complete the following:\n",
    "\n",
    "- Tokenize each sentence into word tokens and retrieve the corresponding **FastText word vectors**.\n",
    "- **Pad or truncate** each sentence to exactly **32 tokens**.\n",
    "- Construct a tensor of shape **(32, 300)** for each sentence (300 = embedding dimension).\n",
    "- **Do not use** `nn.Embedding`; instead, **precompute and batch** the word vectors directly.\n",
    "- Pass the sequences into an **LSTM model** and classify using the **final hidden state**.\n",
    "- Use `nn.CrossEntropyLoss(weight=...)` and evaluate using **macro-averaged F1 score**.\n",
    "\n",
    "#### Performance Requirement:\n",
    "Your model must achieve a **Test Macro F1 Score >= 0.70**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1511ab5",
   "metadata": {},
   "source": [
    "###  Evaluation Requirements **(10 points)**\n",
    "\n",
    "For **both models (MLP and LSTM)**, you must:\n",
    "\n",
    "- Train for **at least 30 epochs**. You may train longer and select the best checkpoint based on validation performance (early stopping is allowed **after** epoch 30).\n",
    "- Track and plot the following metrics for **both training and validation** sets:\n",
    "  - **Loss vs. Epochs**\n",
    "  - **Accuracy vs. Epochs**\n",
    "  - **Macro F1 Score vs. Epochs**\n",
    "\n",
    "Plotting both training and validation curves helps you identify potential issues like **underfitting** or **overfitting**.\n",
    "\n",
    "- After training, evaluate your model on the **test set** and report the **confusion matrix**.\n",
    "- Save plots (training/validation curves and confusion matrix) to disk from your **.py scripts** so they can be embedded in `open_questions.ipynb`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6bc92",
   "metadata": {},
   "source": [
    "## Provided Models (Required) **(12 points)**\n",
    "\n",
    "The following scripts are provided to support comparison between classical baselines and fine-tuned LLMs:\n",
    "\n",
    "- **`train_sentiment_rnn_classifier.py`** - Sentiment classifier using a basic RNN architecture  \n",
    "- **`train_sentiment_gru_classifier.py`** - Sentiment classifier using a GRU architecture  \n",
    "- **`train_sentiment_bert_classifier.py`** - Sentiment classifier using a BERT-based model  \n",
    "- **`train_sentiment_gpt_classifier.py`** - Sentiment classifier using a GPT-based model  \n",
    "\n",
    "You must run these models and include their results in your analysis (metrics, plots, and a brief comparison). BERT and GPT are pretrained LLMs that you will **fine-tune** for classification using these scripts. These scripts are **not** submissions and may use different training settings (e.g., fewer epochs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1293a37",
   "metadata": {},
   "source": [
    "## Open-Ended Reflection Questions **(23 points)**\n",
    "\n",
    "After completing your implementations and running all provided scripts, in the notebook named `open_questions.ipynb` to address the following. You may **Include plots** from your training scripts in the notebook output to justify your answers.\n",
    "\n",
    "### 1. Training Dynamics\n",
    "*Focus on your MLP and LSTM implementations*\n",
    "\n",
    "- Did your models show signs of **overfitting** or **underfitting**? What architectural or training changes could address this?\n",
    "- How did using **class weights** affect training stability and final performance?\n",
    "\n",
    "### 2. Model Performance and Error Analysis\n",
    "*Focus on your MLP and LSTM implementations*\n",
    "\n",
    "- Which of your two models **generalized better** to the test set? Provide evidence from your metrics.\n",
    "- Which **sentiment class** was most frequently misclassified? Propose reasons for this pattern.\n",
    "\n",
    "### 3. Cross-Model Comparison\n",
    "*Compare all six models: MLP, RNN, LSTM, GRU, BERT, GPT*\n",
    "\n",
    "- How did **mean-pooled FastText embeddings** limit the MLP compared to sequence-based models?\n",
    "- What advantage did the LSTM's **sequential processing** provide over the MLP?\n",
    "- Did **fine-tuned LLMs** (BERT/GPT) outperform classical baselines? Explain the performance gap in terms of pretraining and contextual representations.\n",
    "- **Rank all six models** by test performance. What architectural or representational factors explain the ranking?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d2d95",
   "metadata": {},
   "source": [
    "## AI Use Disclosure **(5 points)**\n",
    "\n",
    "Complete the **AI Use Disclosure** section in `open_questions.ipynb`. This item is graded separately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83029649",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "You must submit the following files:\n",
    "\n",
    "1. `train_sentiment_mlp_classifier.py`  \n",
    "   Implementation of **Task 1** using an MLP with **mean-pooled FastText sentence embeddings**.\n",
    "\n",
    "2. `train_sentiment_lstm_classifier.py`  \n",
    "   Implementation of **Task 2** using an LSTM with **padded/truncated FastText word vectors** (32 tokens per sentence).\n",
    "\n",
    "3. `outputs/` containing PNGs for loss/accuracy/F1 curves and confusion matrices for **all models you ran** (MLP, LSTM, RNN, GRU, BERT, GPT).\n",
    "\n",
    "4. `open_questions.ipynb` and `open_questions.html`  \n",
    "   Your written responses to the **open-ended questions** related to modeling choices, performance comparisons, and reflections. The HTML must include the **plots embedded in the notebook output**, plus your **AI Use Disclosure**.\n",
    "\n",
    "Submission Instructions\n",
    "\n",
    "- Submit `open_questions.html` to **Canvas**.\n",
    "- Push **all `.py`, `.ipynb`, `.html`, and `outputs/` files** to your **GitHub repository**.\n",
    "- Make sure the `.html` file contains **both code and output** so it can be viewed without rerunning the notebook.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
